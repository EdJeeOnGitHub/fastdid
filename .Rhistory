dt <- data.table(
id = c(1, 1, 2, 2, 3, 3),
value = c(NA, 10, NA, 25, 30, NA),
value2 = c(1, 2, 3, 4, 5, 6)
)
dt2 <- dt[,.(value = which.min(value),
value2 = value2[which.min(value)]), by = id]
dt <- data.table(
id = c(1, 1, 2, 2, 3, 3),
value = c(NA, 10, 20, 25, 30, NA),
value2 = c(1, 2, 3, 4, 5, 6)
)
dt2 <- dt[,.(value = which.min(value),
value2 = value2[which.min(value)]), by = id]
find_needed_pkg <- function(text, available = ""){
needed <- text %>% pkgDep(suggests = FALSE)
available <- available %>% str_split(",") %>% unlist()
return(needed[!needed %in% available] |> paste(collapse = ","))
}
find_needed_pkg("dplyr")
library(miniCRAN)
library(stringr)
find_needed_pkg <- function(text, available = ""){
needed <- text %>% pkgDep(suggests = FALSE)
available <- available %>% str_split(",") %>% unlist()
return(needed[!needed %in% available] |> paste(collapse = ","))
}
find_needed_pkg("dplyr")
find_needed_pkg("dplyr", "arrow, data.table, stringr, assertthat, bit64, glue, purrr, R6, rlang, tidyselect, vctrs, cpp11, bit, cli, lifecycle, magrittr, withr, stringi")
find_needed_pkg <- function(text, available = ""){
needed <- text %>% pkgDep(suggests = FALSE)
available <- available %>% str_split(",") |> str_trim()%>% unlist()
return(needed[!needed %in% available] |> paste(collapse = ","))
}
find_needed_pkg("dplyr", "arrow, data.table, stringr, assertthat, bit64, glue, purrr, R6, rlang, tidyselect, vctrs, cpp11, bit, cli, lifecycle, magrittr, withr, stringi")
?str_trim
?str_trim
find_needed_pkg <- function(text, available = ""){
needed <- text %>% pkgDep(suggests = FALSE)
available <- available %>% str_split(",")  %>% unlist() |> str_trim()
return(needed[!needed %in% available] |> paste(collapse = ","))
}
find_needed_pkg("dplyr", "arrow, data.table, stringr, assertthat, bit64, glue, purrr, R6, rlang, tidyselect, vctrs, cpp11, bit, cli, lifecycle, magrittr, withr, stringi")
??acero
?str_detect
?inner_join
?expand.grid
?CJ
library(data.table)
CJ
?CJ
?merge
collect_by_ym <- function(query, years = Sys.getenv("START_YEAR"):Sys.getenv("END_YEAR"), only_yearly = FALSE, add = "no", verbose = TRUE){
dt <- data.table()
for(y in years){
for(m in 1:ifelse(only_yearly, 1, 12)){
assign("y", y, envir = parent.env(environment()))
assign("m", m, envir = parent.env(environment()))
new_dt <- eval(enexpr(query), envir = parent.env(environment())) %>% collect() |> as.data.table()
if(add == "time"){
new_dt$TIME <- y*12+m
} else if (add == "ym") {
new_dt$Y <- y
new_dt$M <- m
}
dt <- dt %>% rbind(new_dt)
if(verbose){message(y, " ", m, " done")}
}
}
return(dt)
}
library(fixest)
?feols
library(data.table)
library(rlang)
library(stringr)
library(dplyr)
library(arrow)
#Author: Tsai Lin-tung
# GLOBAL SETTINGs -----------------------------------------
message("Arrow helper: 20230804 ver")
message("Changed: optimized parse_csv_for_arrow, add filenames for selective parsing / converting for csv_to_parquet and parse_csv_for_arrow")
#please change this when running this on a new machine
arrow_path <- "C:/Users/User/Documents/GitHub/nhiHelper/data/parquet"
schema_path <- "C:/Users/User/Documents/GitHub/nhiHelper/arrow/arrow_schema.csv"
parsed_path <- "C:/Users/User/Documents/GitHub/nhiHelper/arrow/arrow_parsed.txt"
Sys.setenv(SCHEMA_PATH = schema_path)
Sys.setenv(ARROW_PATH = arrow_path)
Sys.setenv(PARSED_PATH = parsed_path)
message("setting parquet folder to: ", Sys.getenv("ARROW_PATH"))
message("setting schema path to: ", Sys.getenv("SCHEMA_PATH"))
message("setting parsed path to: ", Sys.getenv("PARSED_PATH"))
# Fix the arrow schema ----------------------------------------------
get_supposed_type <- function(value){
#Takes a value from the error message and try to guess what type the column should be to contain it
#binary: have non-alnum
if(is.na(value)){return("binary")}
if(str_detect(value, "[^a-zA-Z0-9.]+")){return("binary")}
#double: have dot but no alpha
if(str_detect(value, "\\.") & !str_detect(value, "[^0-9.]+")){return("float64")}
#default is string
return("utf8")
}
add_schema_rule <- function(column_name_list, value_type_list, remove_dup = TRUE, message = ""){
#add a schema rule to the disk schema rule list that fix_schema can use
if(length(column_name_list) != length(value_type_list)){stop("Length don't match, please provide type for each variable.")}
for(i in 1:length(column_name_list)){
column_name <- column_name_list[i]
value_type <- value_type_list[i]
schema_path <- Sys.getenv("SCHEMA_PATH")
if(file.exists(schema_path)){
schema_fix <- fread(schema_path)
if(remove_dup){schema_fix <- schema_fix[col_name != column_name]}
} else {schema_fix <- data.table()}
schema_fix <- schema_fix %>% rbind(data.table(col_name = column_name, type = value_type), fill = TRUE)
schema_fix %>% fwrite(schema_path)
message(paste("Changing the type of",column_name, "to", value_type, message))
}
}
parse_csv_for_arrow_file <- function(path, test_line){
#call the add_schema_rule if there's an error to the head, which means arrow can't handle the value
#TODO: through an error if unexpected error mesage come out
if(!str_ends(path, "(?i)csv|parquet|tsv")){warning(path, "does not have a familiar format...")}
dt <- open_dataset_fix_sch(path)
message <- tryCatch(head(dt, n = test_line), error = function(e) e)$message
if(is.null(message)){return(0)} #nothing is wrong
column <- as.numeric(str_remove_all(str_extract(message, "#(\\d+)"), "#")) + 1 #arrow count from 0, not 1........
column_name <- dt$schema[[column]]$name
column_type <- dt$schema[[column]]$type$name
value <- str_remove_all(str_extract(message, "value .*"), paste0(" |value|'|", '"')) #double quote can only be in single quote
value_type <- get_supposed_type(value)
add_schema_rule(column_name, value_type, message = paste("because", column_type, "can't handle", value)) #add the schema rule
return(1)
}
parse_csv_for_arrow <- function(folder, test_line = 50000, max_try = 20, filenames = NULL, overwrite = FALSE){
#go through a whole folder and try to see if arrow can handle it, add schema rules accordingly
error_count <- 0
paths <- list.files(folder, pattern = paste(filenames, collapse = "|"), ignore.case = TRUE, full.names = TRUE)
paths <- paths[str_ends(paths, ".tsv|.csv")]
#files parsed before should be skiped
parsed_path <- Sys.getenv("PARSED_PATH")
if(file.exists(parsed_path)){
parsed_files <- readLines(parsed_path) |> str_split_1(",")
} else {parsed_files <- c()}
for(path in paths){
if (path %in% parsed_files & overwrite == FALSE){
message(path, " is parsed before, skipping.")
next
}
error <- 1
try <- 0
while(error & try < max_try){
error <- parse_csv_for_arrow_file(path, test_line)
error_count <- error_count + error
try <- try + 1
}
if(try != max_try){
message("parsed ", path)
parsed_files <- c(parsed_files, path)
} else {message("failed to parse within try, file: ", path)}
}
#write the parsed filenames down
parsed_files |> str_flatten(collapse = ",")|> writeLines(parsed_path)
message(error_count, " scheme changed.")
}
#change column type based on schema info
fix_scheme <- function(sch){
#fix the scheme using the rules from parse_csv_arrow, or user provided (like mergeing columns)
scheme_table <-data.table()
#find names of scheme
for(i in 1:length(sch)){
f_name <- sch[[i]]$name
f_type <- sch[[i]]$type$name
scheme_table <- scheme_table %>% rbind(data.table(name = f_name, type = f_type, index = i))
}
if(file.exists(Sys.getenv("SCHEMA_PATH"))){
schema_fix <- fread(schema_path)
for(i in 1:nrow(schema_fix)){
col_name <- schema_fix[i, col_name]
type <- schema_fix[i, type]
col <- scheme_table[name == col_name | name == tolower(col_name)]
if(nrow(col) != 1){next}
index <- col[, index]
sch[[index]] <- Field$create(sch[[index]]$name, eval(parse(text = paste0(type, "()"))))
}
}
return(sch)
}
#just a wrapper that fix the scheme before opening
open_dataset_fix_sch <- function(paths){
#basically a wrapper around open_dataset that also does schema fixing
fileformat <- last(str_split(paths, "\\.")[[1]])
if(fileformat == "parquet" | is.na(fileformat)){
head <- open_dataset(paths)
sch <- head$schema
sch <- fix_scheme(sch)
return(open_dataset(paths, schema = sch))
} else {
head <- open_dataset(paths, format = fileformat)
sch <- head$schema
sch <- fix_scheme(sch)
return(open_dataset(paths, schema = sch, format = fileformat, skip = ifelse(fileformat == "parquet", 0, 1)))
}
}
# Create the arrow dataset ----------------------------------------------------------------------
csv_to_parquet_dataset <- function(in_folder, out_folder, overwrite = FALSE, filename, partition){
paths <- list.files(path = in_folder, pattern = filename, ignore.case = TRUE, full.names = TRUE)
dt <- open_dataset_fix_sch(paths) |> group_by(!!rlang::parse_expr(partition))
dt |> write_dataset(out_folder)
}
csv_to_parquet <- function(in_folder, out_folder, overwrite = FALSE, filenames = NULL){
#go through a whole folder and try to turn every file from csv
message("It is recommended to run parse_csv_for_arrow first. Otherwise arrow will fail frequently.")
files <- list.files(in_folder, pattern = paste(filenames, collapse = "|"), ignore.case = TRUE)
files <- files[str_ends(files, ".csv|.tsv")]
for(file in files){
in_path <- paste0(in_folder, "/", file)
filename <- str_split_1(file, "\\.")[1]
out_path <- paste0(out_folder, "/", filename, ".parquet")
fileformat <- str_split_1(file, "\\.")[2]
if(!fileformat %in% c("csv", "tsv")){warning(filename, ".", fileformat, "is not in a familiar format!")}
if(file.exists(out_path) & overwrite == FALSE){
message(paste(filename, "already exists."))
next
}
csv_to_parquet_file(in_path, out_path)
}
}
csv_to_parquet_file <- function(in_path, out_path){
#try both arrow reading and (slower) data.table reading
tryCatch({
dt <- open_dataset_fix_sch(in_path)
dt %>% write_parquet(out_path)
#if error message Error: x must be an object of class 'data.frame', 'RecordBatch', or 'Table', not 'FileSystemDataset'.
#pops, probably mean the arrow version is too old
#https://github.com/apache/arrow/pull/11971
message(paste(out_path, "wrote with arrow."))
},
error = function(e) {
message("Error message: ", e)
message("Arrow failed, try data.table")
tryCatch(
{
dt <- fread(in_path)
dt %>% write_parquet(out_path)
message(paste(out_path, "wrote with data.table"))
},
error = function(e) {
message("Error message: ", e)
message("All failed!")
}
)
}
)
}
# Query the parquet dataset ----------------------------------------------
collect_by_ym <- function(query, years = Sys.getenv("START_YEAR"):Sys.getenv("END_YEAR"), only_yearly = FALSE, add = "no", verbose = TRUE, rename_to_upper = TRUE){
dt <- data.table()
for(y in years){
for(m in 1:ifelse(only_yearly, 1, 12)){
assign("y", y, envir = parent.env(environment()))
assign("m", m, envir = parent.env(environment()))
new_dt <- eval(enexpr(query), envir = parent.env(environment())) %>% collect() |> as.data.table()
if(add == "time"){
new_dt$TIME <- y*12+m
} else if (add == "ym") {
new_dt$Y <- y
new_dt$M <- m
}
dt <- dt %>% rbind(new_dt)
if(verbose){message(y, " ", m, " done")}
}
}
return(dt)
}
#open the relevant arrow datasets with one command!
open_nhi <- function(filename, years = NULL, months = NULL,
parquet_path = Sys.getenv("ARROW_PATH"),
path_only = FALSE,
rename_to_upper = TRUE){
if(any(years > 1911)){stop("Please use ROC year.")}
if(any(months > 12)){stop("Please use valid month.")}
#construct the ym regex
if(is.null(months)){
ym_regex <- ifelse(!is.null(years), str_flatten(years, collapse = "|"), ".")
} else {
months <- ifelse(as.numeric(months) < 10, paste0(0,months), months)
if(is.null(years)){
ym_regex <- months |> paste0("\\.|_") |> str_flatten("|")
} else {
ym_dt <- CJ(y = years, m = months)
ym_dt[, ym := paste0(y,m)]
ym_regex <- str_flatten(ym_dt$ym, collapse = "|")
}
}
paths <- list.files(path = parquet_path, pattern = filename, full.names = TRUE, ignore.case = TRUE)
paths <- str_subset(paths, ym_regex)
if(length(paths) == 0){stop("Didn't find any file!")}
if(path_only){return(paths)}
dt <- open_dataset_fix_sch(paths)
if(rename_to_upper){dt <- dt |> rename_with(toupper)}
return(dt)
}
# Not maintained ------------------------------------
merge_nhi <- function(dt, merged_file, years = NULL, months = NULL, merge_var = NA, pre_filter = TRUE){
#bascially something that automatically get the matching columns
#save a little bit of time I guess
if(is.na(merge_var)){
if(merged_file %in% c("OPDTO", "IPDTO")){
merge_var <- c("FEE_YM", "APPL_TYPE", "HOSP_ID", "APPL_DATE", "CASE_TYPE", "SEQ_NO")
} else if(merged_file == "ENROL"){merge_var <- "ID"}
else {stop("No merge var with no default (default is provided for DTOs, ENROL.)")}
}
to_merge <- open_nhi(merged_file, years = years, months = months)
#need to filter the relevant ones out first, otherwise crash sometimes
if(pre_filter & merged_file == "OPDTE"){
#create the keys
eo_keys <- dt %>% mutate(eo_key = paste0(SEQ_NO, "_", APPL_DATE, "_", HOSP_ID)) %>%
distinct(eo_key) %>% collect() %>% unlist()
#merge with dto
to_merge <- to_merge %>%  mutate(eo_key = paste0(SEQ_NO, "_", APPL_DATE, "_", HOSP_ID)) %>%
filter(eo_key %in% eo_keys) #the filtering
}
merged <- left_join(dt, to_merge, by = merge_var)
return(merged)
}
run_til_success <- function(call, max_try = 10){
#takes a call and run it as a separate file until it returns 0
#need to include needed package/variables since it will be executed in a new session
call <- enquo(call)
call %>% deparse(backtick = TRUE) %>%  str_remove("~") %>% writeLines("code.r")
return_message <- 1
try <- 1
while(return_message != 0 & try < max_try){
message(try, " try.")
return_message <- system("Rscript code.r")
try <- try + 1
}
unlink("code.r")
return(return_message)
}
library(testthat)
source("~/GitHub/nhiHelper/arrow/arrow_helpers.R")
csv_folder <- "C:/Users/User/Documents/GitHub/nhiHelper/data/csv"
arrow_folder <- Sys.getenv("ARROW_PATH")
schema_path <- Sys.getenv("SCHEMA_PATH")
parsed_path <- Sys.getenv("PARSED_PATH")
message("clean all parquet file before starting")
unlink(list.files(arrow_folder, full.names = TRUE))
unlink(schema_path)
parse_csv_for_arrow(csv_folder)
schema <- fread(schema_path)
unlink(parsed_path)
parse_csv_for_arrow(csv_folder)
schema <- fread(schema_path)
csv_to_parquet_dataset
csv_to_parquet_dataset(csv_folder, arrow_path, filename = "opdte", partition = "FEE_YM")
unlink(list.files(arrow_folder, full.names = TRUE))
?unlink(list.files(arrow_folder, full.names = TRUE))
?list.files
unlink(list.files(arrow_folder, full.names = TRUE, recursive = TRUE))
?unlink
unlink(list.files(arrow_folder, full.names = TRUE, recursive = TRUE), recursive = TRUE)
unlink(arrow_folder, recursive = TRUE)
?dir.create
dir.create(arrow_folder)
csv_to_parquet_dataset <- function(in_folder, out_folder, overwrite = FALSE, filename, partition){
paths <- list.files(path = in_folder, pattern = filename, ignore.case = TRUE, full.names = TRUE)
dt <- open_dataset_fix_sch(paths) |> group_by(!!rlang::parse_expr(partition))
dt |> write_dataset(paste0(out_folder, "/", filename))
}
csv_to_parquet_dataset(csv_folder, arrow_path, filename = "opdte", partition = "FEE_YM")
csv_to_parquet_dataset(csv_folder, arrow_path, filename = "OPDTE", partition = "FEE_YM")
opdte <- open_dataset(paste0(arrow_path, "/OPDTE")) |> filter(FEE_YM == "201412") |> summarise(n = n()) |> collect()
opdte <- open_dataset(paste0(arrow_path, "/OPDTE")) |> filter(FEE_YM = "201412") |> summarise(n = n()) |> collect()
op <- open_dataset(paste0(arrow_path, "/OPDTE"))
head(op)
opdte <- open_dataset(paste0(arrow_path, "/OPDTE")) |> filter(FEE_YM == 201412) |> summarise(n = n()) |> collect()
csv_to_parquet_dataset(csv_folder, arrow_path, filename = "OPDTE", partition = "FEE_YM, FUNC_TYPE")
csv_to_parquet_dataset(csv_folder, arrow_path, filename = "OPDTE", partition = "FEE_YM,FUNC_TYPE")
csv_to_parquet_dataset <- function(in_folder, out_folder, overwrite = FALSE, filename, partition){
paths <- list.files(path = in_folder, pattern = filename, ignore.case = TRUE, full.names = TRUE)
dt <- open_dataset_fix_sch(paths) |> group_by(!all_of(partition))
dt |> write_dataset(paste0(out_folder, "/", filename))
}
csv_to_parquet_dataset(csv_folder, arrow_path, filename = "OPDTE", partition = c("FEE_YM", "FUNC_TYPE"))
csv_to_parquet_dataset(csv_folder, arrow_path, filename = "OPDTE", partition = "FEE_YM")
csv_to_parquet_dataset <- function(in_folder, out_folder, overwrite = FALSE, filename, partition){
paths <- list.files(path = in_folder, pattern = filename, ignore.case = TRUE, full.names = TRUE)
dt <- open_dataset_fix_sch(paths) |> group_by(all_of(partition))
dt |> write_dataset(paste0(out_folder, "/", filename))
}
csv_to_parquet_dataset <- function(in_folder, out_folder, overwrite = FALSE, filename, partition){
paths <- list.files(path = in_folder, pattern = filename, ignore.case = TRUE, full.names = TRUE)
dt <- open_dataset_fix_sch(paths) |> group_by(all_of(partition))
dt |> write_dataset(paste0(out_folder, "/", filename))
}
csv_to_parquet_dataset(csv_folder, arrow_path, filename = "OPDTE", partition = "FEE_YM")
?parse_expr
csv_to_parquet_dataset <- function(in_folder, out_folder, overwrite = FALSE, filename, partition){
paths <- list.files(path = in_folder, pattern = filename, ignore.case = TRUE, full.names = TRUE)
dt <- open_dataset_fix_sch(paths) |> group_by(!!rlang::parse_exprs(partition))
dt |> write_dataset(paste0(out_folder, "/", filename))
}
csv_to_parquet_dataset(csv_folder, arrow_path, filename = "OPDTE", partition = c("FEE_YM", "FUNC_TYPE"))
csv_to_parquet_dataset <- function(in_folder, out_folder, overwrite = FALSE, filename, partition){
paths <- list.files(path = in_folder, pattern = filename, ignore.case = TRUE, full.names = TRUE)
dt <- open_dataset_fix_sch(paths) |> group_by(!!rlang::parse_expr(partition))
dt |> write_dataset(paste0(out_folder, "/", filename))
}
csv_to_parquet_dataset(csv_folder, arrow_path, filename = "OPDTE", partition = "FEE_YM")
open_nhi_dt <- function(filename, parquet_path = Sys.getenv("ARROW_PATH")){
return(open_dataset(paste0(parquet_path, "/", filename)))
}
op <- open_nhi_dt("OPDTE")
op |> summarize(n = n()) |> collect()
sum <- op |> summarize(n = n()) |> collect()
sum$n
test_that("test arrow full dataset",{
unlink(arrow_folder, recursive = TRUE)
dir.create(arrow_folder)
csv_to_parquet_dataset(csv_folder, arrow_path, filename = "OPDTE", partition = "FEE_YM")
op <- open_nhi_dt("OPDTE")
sum <- op |> summarize(n = n()) |> collect()
expect_equal(sum$n, 1667307)
})
text <- "| NUM | FUNC_TYPE | COUNT_MEAN | YEARLY_RATIO | QUARTERLY_RATIO | PAIR_COUNT |
|-----|-----------|------------|--------------|-----------------|------------|
|   1 |        2B |       7.06 |         0.62 |            0.33 |        381 |
|   2 |        AB |       3.94 |         0.34 |            0.16 |      11723 |
|   3 |        AE |       3.63 |         0.33 |            0.14 |       7492 |
|   4 |        2A |       3.19 |         0.40 |            0.09 |        385 |
|   5 |        13 |       2.91 |         0.31 |            0.07 |       5159 |
|   6 |        12 |       2.88 |         0.28 |            0.09 |       7085 |
|   7 |        AD |       2.87 |         0.39 |            0.01 |       1806 |
|   8 |        AG |       2.76 |         0.29 |            0.08 |       2500 |
|   9 |        07 |       2.72 |         0.30 |            0.06 |       1100 |
|  10 |        AF |       2.63 |         0.22 |            0.08 |        790 |
|  11 |        14 |       2.62 |         0.27 |            0.06 |      8420 |
|  12 |        00 |       2.42 |         0.27 |            0.04 |    324169 |
|  13 |        BB |       2.24 |         0.51 |            0.00 |       441 |
|  14 |        02 |       2.20 |         0.21 |            0.04 |    189814 |
|  15 |        04 |       2.20 |         0.25 |            0.03 |     36855 |
|  16 |        08 |       2.20 |         0.17 |            0.04 |     11668 |
|  17 |        60 |       2.16 |         0.22 |            0.03 |    198662 |
|  18 |        01 |       2.12 |         0.21 |            0.03 |    166693 |
|  19 |        05 |       1.98 |         0.18 |            0.03 |     27546 |
|  20 |        AC |       1.97 |         0.16 |            0.04 |      2575 |
|  21 |        10 |       1.94 |         0.18 |            0.02 |     25542 |
|  22 |        BA |       1.93 |         0.21 |            0.02 |      6776 |
|  23 |        AA |       1.81 |         0.22 |            0.01 |      1906 |
|  24 |        11 |       1.78 |         0.16 |            0.01 |     65444 |
|  25 |        BD |       1.76 |         0.14 |            0.02 |      1338 |
|  26 |        06 |       1.66 |         0.12 |            0.01 |     36492 |
|  27 |        09 |       1.65 |         0.14 |            0.01 |     64460 |
|  28 |        40 |       1.62 |         0.14 |            0.00 |     75909 |
|  29 |        03 |       1.49 |         0.10 |            0.01 |     73380 |
|  30 |        22 |       1.09 |         0.01 |            0.00 |      3556 |"
library(stringr)
text2 <- text |> str_trim()
text2
text2 <- text |> str_trim(side = "both")
9e9
?glm
?lm
?interaction
library(cem)
data(LL)
todrop <- c("treated","re78")
imbalance(LL$treated, LL, drop=todrop)
mat <- cem(treatment="treated", data=LL, drop="re78")
mat$strata
library(data.table)
library(stringr)
setwd("C:/Users/User/Documents/GitHub/nhiHelper")
icd_ccs <- fread("icd/icd_ccs/icd_ccs.csv")
View(icd_ccs)
icd_ccs_9 <- fread("icd/icd_ccs/icd9_raw.csv")
View(icd_ccs_9)
library(rdrobust)
?rdrobust
rm(list = ls())
gc()
library(profvis)
setwd("~/GitHub/EventStudyCode")
source("sim_did.R")
source("source/eventcode.R")
setDTthreads(0)
options(kit.nThread = getDTthreads())
setFixest_nthreads(getDTthreads())
simdt <- sim_did(100000, 10, cov = "int", hetero = "dynamic")
dt <- simdt$dt
# event code ---------------------------------------------------------------------
profvis({
event_panel <- copy(dt) #copying so that the original does not change
min_time <- -Inf
max_time <- Inf
y_name <- "y"
t_name <- "time"
unit_name <- "unit"
cohort_name <- "G"
balance_covariate <- "x"
event_code_est <- event_panel %>% event_code(timevar = t_name, unitvar = unit_name,
outcomevar = y_name,
cohortvar = cohort_name,
covariate_base_balance = balance_covariate,
lower_event_time = min_time,
upper_event_time = max_time,
never_treat_action = "both")
})
?chatToFact
?charToFact
